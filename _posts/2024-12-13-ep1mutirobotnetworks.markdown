---
layout: post
comments: true
title: "#1 - Sandilya Sai Garimella: Multi-Robot Networks"
excerpt: "Sandilya's work focuses on multi-robot networks and contact perception using graph neural networks. His research aims to improve how robots coordinate and understand their environment, potentially leading to more versatile and capable robotic systems."
date: 2024-12-13 13:12:00
mathjax: false
---

<style>
.post-header h1 {
    font-size: 35px;
}
.post pre,
.post code {
    background-color: #fcfcfc;
    font-size: 13px;
}
.post blockquote {
    font-style: italic;
    background: #f9f9f9;
    border-left: 5px solid #ccc;
    margin: 1.5em 10px;
    padding: 0.5em 10px;
}
</style>

In this episode, we delve into the intersection of robotics, biomimicry, and multi-agent systems. Drawing inspiration from nature, we explore how decentralized coordination in schools of fish and swarms of drones can revolutionize real-world applications like environmental monitoring and logistics. We discuss the trade-offs between centralized and decentralized networks, the diminishing returns of adding agents, and the challenges of bridging the gap between simulation and real-world robotics. This conversation unpacks how nature’s efficiency and adaptability inspire cutting-edge robotic designs.

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/episode/0XHV07vqsajxBW9znfMz0P?utm_source=generator&t=0" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>

**Bio-inspired multi-agent systems.** Bio-inspired multi-agent systems draw inspiration from nature, where groups of animals can achieve far better results than individual units. This phenomenon is observed in various species, such as ants, bees, and fish. A striking example is a school of fish evading predators. Hundreds of fish coordinate effortlessly to avoid danger, while dolphins must develop separate strategies as a multi-agent system to capture these schools. Interestingly, individual fish do not coordinate with every other fish in the school; instead, they focus on their nearest neighbors. If we conceptualize these schools of fish as a graph, with nodes representing each fish, we would notice that it is not a fully connected mesh. This structure is efficient because communicating with every other fish in the school would be expensive and time-consuming, creating a trade-off between guaranteed performance (successfully evading the predator each time) and real-time responsiveness. In my research, these systems are called decentralized multi-agent networks, where each agent does not necessarily communicate with every other agent. Instead, agents focus on their immediate neighborhood. Through this localized coordination, the system achieves a level of coordination comparable to that of a fully centralized system. This approach offers several advantages: 1) reduced communication overhead, 2) improved scalability, 3) enhanced real-time performance, and 4) robustness to individual agent failures. By mimicking nature's decentralized coordination strategies, these multi-agent systems can efficiently solve complex problems in various fields, including robotics, logistics, and environmental monitoring.

**Difference between centralization and decentralization.** Centralization occurs when the network is fully connected, with robots receiving information from every member of the group. In contrast, decentralization refers to a network that is fully disconnected, where robots receive no information from others in the group. While full centralization might lead to solutions closer to optimal, it has significant drawbacks: 1) optimal guarantees: centralized networks offer better optimality guarantees. For instance, in map exploration tasks, a fully centralized network can provide stronger assurances about area coverage within a given timeframe; 2) real-time performance issues: the primary reason for moving away from full centralization is its real-time performance shortcomings, particularly communication delays; 3) bandwidth limitations: with multiple robots collecting rich, sensitive data streams (RGB, depth, LIDAR, 3D points), transmitting this information over a centralized network introduces significant delays due to fixed bandwidth constraints. Two common centralized approaches include - 1) peer-to-peer communication: vehicles directly communicate with each other; 2) base station model: all robots transmit data to a central base station, which then distributes information among the robots. The base station model requires a powerful host machine with substantial computing capabilities. However, this centralized approach is more vulnerable to attacks. If an adversarial agent compromises the base station, it could potentially ground the entire fleet of robots. The focus on distributed/decentralized networks offers several benefits: 1) resilience: if one robot fails, others can continue operating; 2) scalability: easier to add or remove robots from the network; 3) reduced vulnerability: no single point of failure that could compromise the entire system. By prioritizing decentralized networks, we can create more robust and adaptable multi-robot systems that can maintain functionality even in the face of individual robot failures or compromises.

**Critical point - diminishing returns with more centralization.** A critical consideration in multi-robot systems is the concept of diminishing returns with increased centralization, particularly when addressing tasks like area coverage in an unknown map. In this scenario, robots are tasked with dynamically coordinating their movements to maximize area coverage in the shortest time possible. For instance, imagine two robots (or humans) working to explore a room. To avoid overlapping efforts, one robot might cover the first half of the room while the other moves as far away as possible to explore the remaining area. By stitching their individual maps together, they create a comprehensive global map with minimal redundancy. When additional robots are introduced into the system, their marginal utility — the additional benefit gained from their observations — begins to decrease. This phenomenon reflects diminishing returns, where each new robot contributes less to the overall task. If we were to plot this relationship on a graph, the horizontal axis represents the number of robots and the vertical axis represents marginal utility. As more robots are added, the curve flattens, indicating that the incremental benefit decreases with each additional robot. This diminishing return is tied to the concept of submodularity, which describes how adding more resources (e.g., robots) yields progressively smaller benefits in certain tasks, such as area coverage or SLAM (Simultaneous Localization and Mapping). The insight here is that after a certain point, adding more robots does not significantly improve performance. Comparable results can often be achieved with fewer robots by efficiently exploiting submodularity. The “punchline” of this research emphasizes that there is a critical point where increasing the number of robots no longer provides substantial benefits. Understanding and leveraging this principle allows for more efficient system design, ensuring that resources are not wasted on unnecessary additions while still achieving optimal performance.

**Drone warfare - cost of not moving beyond your neighbor.** In the context of drone warfare, a critical consideration is the cost of not moving beyond your immediate neighbors when planning trajectories for optimal data collection and environmental exploration. When operating in a dynamic environment, it is essential to gather data from drones that are positioned further away. If there are potential overlaps in trajectories over a finite planning horizon, selecting paths that minimize this overlap becomes crucial for effective exploration. This necessitates moving beyond immediate neighbors, which are typically just one hop away. In our research, we do not explicitly communicate with neighbors outside of the designated communication range. This decision is based on practical considerations for real-world implementation, where the communication range of drones can vary from a few hundred meters to up to a kilometer. If a drone is 2 kilometers away or beyond the communication range, it may not be feasible to exchange information, even if desired. As drones move and opportunistically enter communication range, each drone must decide which subset of its candidate neighbors to coordinate with. This approach limits the number of in-neighbors for each drone, transitioning from a completely decentralized system to one that allows coordination with up to six neighbors. This strategy emphasizes the importance of optimizing communication and coordination among drones in a multi-agent system. By selectively choosing which neighbors to engage with, drones can enhance their operational efficiency while effectively exploring their environment and minimizing trajectory overlaps.

**Sim-to-Real Gap.** The sim-to-real gap presents significant challenges when transitioning from simulated drone swarms to real-world implementations. While simulations can artificially induce communication delays and model signal attenuation, real-world scenarios introduce far more complex variables: 1) actual radio hardware is required for vehicle-to-vehicle communication, and 2) off-the-shelf solutions or custom-designed systems (like those in the DARPA subterranean challenge) may be necessary. Simulated sensor data often fails to accurately represent real-world conditions: 1) camera and IMU noise may not follow Gaussian distributions as often assumed in simulations; 2) real sensors may have different characteristics or limitations compared to their simulated counterparts. Implementing a large-scale drone swarm presents significant logistical and financial challenges: 1) equipping numerous drones (15 or 45) with advanced sensors like RGBD cameras, depth sensors, and gimbals is costly; 2) acquiring and maintaining such a fleet requires substantial investment. Real-world experiments introduce a host of operational challenges not present in simulations: 1) extensive ROS1 or ROS2 code is needed to coordinate takeoff, task execution, and landing for the entire fleet; 2) contingency plans must be developed for scenarios like drone damage or crashes; 3) safety considerations become paramount in physical implementations. Designing and executing a real-life experiment with a drone swarm would be a significant achievement, requiring: 1) robust automation systems to manage the fleet; 2) comprehensive safety protocols; 3) strategies for handling unexpected situations or failures. The transition from simulation to reality in drone swarm coordination represents a complex and multifaceted challenge, requiring careful consideration of hardware, software, safety, and operational factors.

**The two paths ahead for robots.** There is indeed a connecting thread between the two projects, including the first article I wrote. The higher-level idea behind these projects, including the multi-robot networks with decentralized control, is to democratize access to robots. In the larger context, we see ChatGPT and other GPT models becoming widely accessible to anyone with an internet connection, primarily addressing the reasoning side of artificial intelligence. This situation highlights Moravec's paradox, which states that robots find reasoning tasks relatively simple, while humans excel at sensory-motor skills. For instance, chess engines can quickly formulate optimal moves and achieve ELO ratings far surpassing even the best grandmasters. However, robots struggle with manipulation and tangible interaction with the environment, tasks that humans find almost trivial. Conversely, humans require extensive training to match the mental capabilities of machines in certain areas. The legged robot project aimed to leverage the strengths of both humans and robots. We used a six-legged robot constructed entirely from a special kind of styrofoam, like insulation foam but thinner. This approach significantly reduced costs, with the robot costing roughly $1,500 compared to more advanced models like Boston Dynamics' Spot, which could cost around $30,000 per unit. While our robot lacked the advanced features of Spot, such as 3D LIDAR and depth cameras, it could still move and be fully controlled in the 2D plane. The goal was to explore its potential use in agricultural settings, where large crop harvesters often trample viable crop areas. The specific scientific question focused on designing this robot to control lawn pests, particularly dandelions, without complex mechanisms for picking them. We integrated perception using a lighter camera with RGB streams. The project aimed to create a cost-effective solution that, while not as productive as more expensive robots, could still be fairly effective. One key innovation was utilizing the robot's natural walking motion to pick dandelions without additional appendages. The hexapedal robot uses an alternating tripod gait inspired by insects, which produces a slight parasitic vertical motion. I designed a trajectory that allows the robot to pick up objects once it reaches a target, eliminating the need for extra moving parts. This project also had a strong educational motive, providing experience in integrating perception and control aspects in robotics.

**Biomimicry in robots.** Biomimicry in robotics draws inspiration from nature's proven designs, offering efficient and effective solutions for complex problems. This approach benefits robotics in several ways: 1) proven concepts: nature has already developed and refined solutions through evolution; efficiency: natural designs are often not over-engineered, providing just enough functionality for the required tasks; adaptability: bio-inspired designs can be augmented and enhanced for specific robotic applications. MIHGNN, or Morphology Informed Heterogeneous Graph Neural Network, is an approach used in legged robotics for contact perception and force prediction. Key aspects include: 1) structure: mirrors the kinematic structure of the robot using a graph neural network; 2) sensors: nodes in the graph represent force sensors on the robot's feet and torque sensors at each joint; 3) training: uses data collected from various terrains (flat, sloping, rough, stairs). Contact perception aims to measure forces experienced by each leg during stance or natural gait. It aims to predict future forces on the legs based on the current state. While MIHGNN and similar approaches show promise, they face challenges in generalizing to completely unknown terrains: 1) training requirements: extensive training on various terrains is necessary; 2) limitations: out-of-sample performance on entirely new terrains remains a challenge; 3) complexity: contact perception and friction modeling involve highly non-linear and complex dynamics. An example of generalization is when a robot trained on sand may adapt to gravel or finer sand due to similar slippage characteristics. However, it may struggle in vastly different environments like forests with leaves, unless specifically trained for such conditions.

**GNNs’ computational challenges.** Graph Neural Networks (GNNs) in the field of contact perception face computational challenges, but their future remains promising due to several factors. The continuous increase in computational power, as predicted by Moore’s law, is expected to benefit GNNs in two key ways: 1) faster online inference: as computers become more powerful, the real-time processing capabilities of GNNs will improve, making them more practical for applications like contact perception; 2) enhanced training capabilities: more powerful computers will allow for training on larger datasets in the same amount of time, potentially improving the performance and generalization of GNNs. One significant bottleneck for GNN development in robotics is the need for extensive real-world data: 1) diverse terrain data: to generalize well, GNNs require data from a wide range of terrains, including those that might be considered "almost seen"; 2) access to robots: collecting real-life datasets requires access to physical robots, which can be a limiting factor for many researchers; 3) common platform: there’s a need for a standardized platform for encoding GNNs to facilitate collaboration and data sharing among researchers. GNNs offer some advantages over traditional high-fidelity dynamics models: 1) faster real-time inference: while GNNs may take longer to train, they often provide faster real-time inference compared to complex dynamics models; 2) performance in time-critical applications: in scenarios like path planning for drones or collision avoidance, the quicker inference time of GNNs can be crucial for avoiding accidents.

**Interdisciplinary approach.** So, coming from a mechanical and electrical engineering background where I mainly focused on dynamics and control, jumping into robotics was quite a challenge, especially when it came to programming. I have to admit, this was before the ChatGPT era - we're talking Stack Exchange times here. I still love Stack Exchange, by the way. Anyway, I found the learning curve for programming pretty steep, but I was really lucky to have a graduate student mentor and another professor who gave me advice and feedback to help me improve. The big thing was building up my programming skills, moving from MATLAB (which a lot of people think of as a lesser language, but that's a whole other conversation) to C++. I did this mainly through a master's program, but if you're coming from an undergrad background, that should be enough. The key thing for me was realizing how interdisciplinary robotics is. Instead of seeing tools as ideologies, I tried to view them simply as tools to solve problems. This way, you don't pigeonhole yourself into one way of solving things - you look at what you can adapt or use from different areas. That's super useful. Another crucial thing is having a solid math background. Robotics is vast, but linear algebra is probably one of the most important areas to be strong in. Back in my undergrad days, I used to mostly ask "why" questions like "Why is Bayesian optimization optimal?" and stuff like that. But when I started my PhD, I really tried to spend more time with pen and paper, reading textbooks or online resources, and focusing on the fundamental math concepts and theorems, working through examples. I also worked with a mentor in the PhD program, and that helped me a ton. This broad range of skills and the ability to pull from different disciplines is what makes robotics so exciting and challenging.

**Motivating questions.** What we're trying to do with artificial intelligence and robotics is to augment human capabilities and gain insights that a team of humans alone might not have. It's not about completely replacing humans, but giving them an edge, increasing productivity, or saving time to tackle real-world problems. I focus on things like search and rescue, exploration, and persistent surveillance. The main thing that motivates me is seeing an abstract idea - something initially just on a whiteboard or scribbled on the back of an envelope - come to fruition. Seeing that idea manifest in a real robot doing an actual task, or in a simulation with guarantees that can transfer to real robots, is incredibly satisfying. It's amazing that with our current resources, we're still able to make these advancements. We're not necessarily doing this through huge leaps or paradigm shifts. We don't need a huge overnight paradigm shift to have a profound impact. It's about using and optimizing existing concepts to increase the capabilities of existing machines. Those huge leaps do come along with that process, but what really motivates me is seeing those incremental improvements add up to something significant.

> More of a visual learner? There's a YouTube video too. Click [here](https://www.youtube.com/@gatechai).

~<br>
*Sandilya Sai Garimella is a graduate researcher in the field of robotics and artificial intelligence here at Georgia Tech. He’s worked closely on one of the crucial challenges in legged robotics: contact perception. Using graph neural networks, he is helping the field reconsider how robots understand and interact with their environment. Imagine a world where robots can navigate in any terrain as effortlessly as animals do. Sandilya’s research could one day make this a reality. And the best part is… his work doesn’t not stop there. It extends to multi-robot networks, exploring how systems can coordinate efficiently to tackle complex tasks, such as drone surveillance. What sets Sandilya apart is his multidisciplinary approach. He’s not just developing algorithms from an ivory tower. He’s reimagining how robots can learn and adapt in real-world scenarios. Beyond his research, we’ll also touch on the practical challenges of implementing these cutting-edge technologies and what it means for the future of industries ranging from agriculture to urban planning.*